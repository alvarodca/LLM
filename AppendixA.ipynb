{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4742dcf3",
   "metadata": {},
   "source": [
    "# APPENDIX A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a104bca",
   "metadata": {},
   "source": [
    "This following file covers the initiations and basics of pytorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb330241",
   "metadata": {},
   "source": [
    "**A.1 What is Pytorch?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b91599",
   "metadata": {},
   "source": [
    "Useful due to its dealing with tensors, it automatically computes gradients for tensor operations and it has many built in loss functions and optimizers.\n",
    "Deep learning is just a type of machine learning\n",
    "\n",
    "Initializing it on the terminal\n",
    "-pip install pytorch\n",
    "-pip3 install torch torchvision torchaudio\n",
    "-pip show torch should return version 2.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1822880d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3715335f",
   "metadata": {},
   "source": [
    "**A.2 Understanding tensors?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32c2acb",
   "metadata": {},
   "source": [
    "Tensors are a generalization of matrices to higher dimensions\n",
    "They are data containers for array-like structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53e342b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 0 dimensional tensor\n",
    "tensor0d = torch.tensor(1)\n",
    "\n",
    "# 1 dimensional tensor\n",
    "tensor1d = torch.tensor([1,2,3])\n",
    "\n",
    "# 2 dimensional tensor\n",
    "tensor2d = torch.tensor([[1,2],[3,4]])\n",
    "\n",
    "# 3 dimensional tensor\n",
    "tensor3d = torch.tensor([[[1,2],[3,4]],[[5,6],[7,8]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13b0224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64 torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Data Types, they are both 64-bit integers, 64 bits leads to more precision although it causes a larger memory consumption.\n",
    "print(tensor0d.dtype, tensor1d.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6aa834f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "torch.Size([2, 2])\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6],\n",
      "        [7, 8]])\n",
      "tensor([[[1, 5],\n",
      "         [3, 7]],\n",
      "\n",
      "        [[2, 6],\n",
      "         [4, 8]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_29548\\3521843475.py:14: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3701.)\n",
      "  print(tensor3d.T)\n"
     ]
    }
   ],
   "source": [
    "# Operations\n",
    "\n",
    "# Obtaining the tensor\n",
    "print(tensor0d)\n",
    "\n",
    "# Obtaining the size\n",
    "print(tensor2d.shape)\n",
    "\n",
    "# Reshaping the tensor\n",
    "print(tensor3d.reshape(4,2))\n",
    "# .view is more common on this case\n",
    "\n",
    "# Transposing the tensor\n",
    "print(tensor3d.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63538486",
   "metadata": {},
   "source": [
    "**A.3 Seeing models as computation graphs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213f1dcf",
   "metadata": {},
   "source": [
    "Autograd is a built in function of Torch which computes gradients automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a307596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0852)\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression classifier\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "y = torch.tensor([1.0]) # label\n",
    "x1 = torch.tensor([1.1]) # input\n",
    "w1 = torch.tensor([2.2]) # weight\n",
    "b = torch.tensor([0.0]) # bias\n",
    "z = x1 * w1 + b # formula\n",
    "a = torch.sigmoid(z) # activation formula, any number is squashed between 0 and 1.\n",
    "loss = F.binary_cross_entropy(a, y) # output, how wrong the prediction is\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ac4f61",
   "metadata": {},
   "source": [
    "**A.4 Automatic Differentiation Made Easy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e219c46e",
   "metadata": {},
   "source": [
    "The attribute requires_grad set to True will build a computational graph internally, this is useful if we want to compute gradients.\n",
    "Gradients are computed with partial derivatives, done using the chain rule from right to left in the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac61dae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-0.0898]),)\n",
      "(tensor([-0.0817]),)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.0817])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing gradients with autograd\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "\n",
    "\n",
    "y = torch.tensor([1.0])\n",
    "x1 = torch.tensor([1.1])\n",
    "w1 = torch.tensor([2.2], requires_grad=True) # Parameter requires grad set to True\n",
    "b = torch.tensor([0.0], requires_grad=True)\n",
    "z = x1 * w1 + b\n",
    "a = torch.sigmoid(z)\n",
    "loss = F.binary_cross_entropy(a, y)\n",
    "\n",
    "grad_L_w1 = grad(loss, w1, retain_graph=True) # Loss is a scalar value representing the model's error.\n",
    "grad_L_b = grad(loss, b, retain_graph=True) # Retain graph maintains the gradient in memory, useful if we wish to use it later\n",
    "\n",
    "print(grad_L_w1)\n",
    "print(grad_L_b)\n",
    "\n",
    "# loss.backward() does gradient computation for all parameters that have requires_grad at once. Store in .grad attributes\n",
    "loss.backward()\n",
    "b.grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f542d5a1",
   "metadata": {},
   "source": [
    "**A.5 Implementing Multilayer Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e381b07",
   "metadata": {},
   "source": [
    "The subclass torch.nn.Module class is used to define our own network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4be7af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=50, out_features=30, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=30, out_features=20, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "Parameter containing:\n",
      "tensor([[ 0.0502,  0.0307,  0.0333,  ...,  0.0951,  0.1134, -0.0297],\n",
      "        [ 0.1077, -0.1108,  0.0122,  ...,  0.0108, -0.1049, -0.1063],\n",
      "        [-0.0920, -0.0480,  0.0105,  ..., -0.0923,  0.1201,  0.0330],\n",
      "        ...,\n",
      "        [ 0.1359,  0.0175, -0.0673,  ...,  0.0674,  0.0676,  0.1058],\n",
      "        [ 0.0790,  0.1343, -0.0293,  ...,  0.0344, -0.0971, -0.0509],\n",
      "        [-0.1250,  0.0513,  0.0366,  ..., -0.1370,  0.1074, -0.0704]],\n",
      "       requires_grad=True)\n",
      "torch.Size([30, 50])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1eb52a88110>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initiating a  multilayer perceptron with two hidden layers\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential( # Sequential are used as layers follow a straight path in this case\n",
    "\n",
    "        # 1st hidden layer\n",
    "        torch.nn.Linear(num_inputs, 30), # Linear Layers takes the number of inputs and outputs in this case 30\n",
    "        torch.nn.ReLU(),  # Nonlinear activation functions\n",
    "        # 2nd hidden layer\n",
    "        torch.nn.Linear(30, 20), # The inputs have to match the previous outputs\n",
    "        torch.nn.ReLU(),\n",
    "        # output layer\n",
    "        torch.nn.Linear(20, num_outputs), # Final layer matching the current input to the desired ones\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits\n",
    "\n",
    "# Initiating a neural network object\n",
    "model = NeuralNetwork(50,3)\n",
    "\n",
    "print(model)\n",
    "\n",
    "# Due to our model being sequential, we can call self.layers for all of them \n",
    "\n",
    "# Our first layer as seen on the previous output is at position index 0\n",
    "print(model.layers[0].weight)\n",
    "print(model.layers[0].weight.shape)\n",
    "\n",
    "# To ensure reproducibility, we can use torch.manual_seed(123)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e12a5a4",
   "metadata": {},
   "source": [
    "Each of the neurons has a weight parameter randomly initialized to a low number. This is because if it were the same everytime, the same gradients would be constantly calculated and the update will be identical as well. Moreover, they are low number to ensure that gradients are not too large causing them to explode, although they should not be too smal as this would cause the gradients to vanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d208552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0879,  0.1729,  0.1534]], grad_fn=<AddmmBackward0>)\n",
      "prediction: tensor([[-0.0879,  0.1729,  0.1534]])\n",
      "softmax: tensor([[0.2801, 0.3635, 0.3565]])\n"
     ]
    }
   ],
   "source": [
    "# Simple training example\n",
    "torch.manual_seed(123)\n",
    "X = torch.rand((1, 50))\n",
    "out = model(X)\n",
    "print(out)\n",
    "\n",
    "# If we were to predict, not train the model\n",
    "with torch.no_grad():\n",
    " out = model(X)\n",
    "print(\"prediction:\",out)\n",
    "# In this way, gradients are not stored, saving both memory and computation\n",
    "\n",
    "# To obtain the probabilities, we use softmakx activation function\n",
    "with torch.no_grad():\n",
    " out = torch.softmax(model(X), dim=1)\n",
    "print(\"softmax:\",out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c6ffd9",
   "metadata": {},
   "source": [
    "**A.6 Setting up efficient data loaders**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60968dd5",
   "metadata": {},
   "source": [
    "Crucial for handling training and testing data.\n",
    "A dataset calss will be created to handle training and testing, after that, the data loaders will be created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c409624b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a small toy dataset\n",
    "X_train = torch.tensor([\n",
    " [-1.2, 3.1],\n",
    " [-0.9, 2.9],\n",
    " [-0.5, 2.6],\n",
    " [2.3, -1.1],\n",
    " [2.7, -1.5]\n",
    "])\n",
    "y_train = torch.tensor([0, 0, 0, 1, 1])\n",
    "X_test = torch.tensor([\n",
    " [-0.8, 2.8],\n",
    " [2.6, -1.6],\n",
    "])\n",
    "y_test = torch.tensor([0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0739aa6a",
   "metadata": {},
   "source": [
    "Now we will create a toy dataset using the already built in dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "414b961d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    " def __init__(self, X, y): \n",
    "    self.features = X\n",
    "    self.labels = y\n",
    "\n",
    " def __getitem__(self, index): # Used to obtain one data example and its label\n",
    "    one_x = self.features[index]\n",
    "    one_y = self.labels[index]\n",
    "    return one_x, one_y\n",
    " \n",
    " def __len__(self):\n",
    "    return self.labels.shape[0] # Returns the data total length\n",
    " \n",
    "train_ds = ToyDataset(X_train, y_train)\n",
    "test_ds = ToyDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfb8593",
   "metadata": {},
   "source": [
    "With this done, we can now proceed to initiate the DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8898cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: tensor([[ 2.3000, -1.1000],\n",
      "        [-0.9000,  2.9000]]) tensor([1, 0])\n",
      "Batch 2: tensor([[-1.2000,  3.1000],\n",
      "        [-0.5000,  2.6000]]) tensor([0, 0])\n",
      "Batch 3: tensor([[ 2.7000, -1.5000]]) tensor([1])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    " dataset=train_ds, # Uses our custom data\n",
    " batch_size=2, # Loads two training instances at a time\n",
    " shuffle=True, # Shuffles data at each epoch to avoid bias\n",
    " num_workers=0 # Subprocesses loading in parallel\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    " dataset=test_ds,\n",
    " batch_size=2,\n",
    " shuffle=False, # Data is not shuffled on testing as results are determinsitic, it keeps evaluation consistent\n",
    " num_workers=0\n",
    ")\n",
    "\n",
    "# Iterating over our data\n",
    "for idx, (x, y) in enumerate(train_loader):\n",
    " print(f\"Batch {idx+1}:\", x, y)\n",
    "\n",
    "# As we have 5 instances and we specified a batch size of 2, our last batch has half the number of tensors.\n",
    "train_loader = DataLoader(\n",
    " dataset=train_ds,\n",
    " batch_size=2,\n",
    " shuffle=True,\n",
    " num_workers=0,\n",
    " drop_last=True\n",
    ")\n",
    "# This will drop the las batch as it can affect training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8701fbd",
   "metadata": {},
   "source": [
    "**A.7 A typical training loop**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1c4b16",
   "metadata": {},
   "source": [
    "In this simple case, we will use Stochastic Gradient Descent, while there are many others, this is just a simple explanation of what this optimizer is.\n",
    "\n",
    "Stochastic means random, instead of computing the gradient will all the data it takes a small subset which can be a single instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc1ef44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/003 | Batch 000/002 | Train Loss: 0.75\n",
      "Epoch: 001/003 | Batch 001/002 | Train Loss: 0.65\n",
      "Epoch: 002/003 | Batch 000/002 | Train Loss: 0.44\n",
      "Epoch: 002/003 | Batch 001/002 | Train Loss: 0.13\n",
      "Epoch: 003/003 | Batch 000/002 | Train Loss: 0.03\n",
      "Epoch: 003/003 | Batch 001/002 | Train Loss: 0.00\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "model = NeuralNetwork(num_inputs=2, num_outputs=2)\n",
    "\n",
    "optimizer = torch.optim.SGD( # Optimizer is Stochastic Gradient Descent\n",
    " model.parameters(), lr=0.5 # Specifying which parameters to optimize, lr <- learning rate\n",
    ")\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    " model.train() # Set to training, important as components such as dropouts behave differently if not\n",
    "\n",
    " for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "    logits = model(features) # Obtaining the output\n",
    "\n",
    "    loss = F.cross_entropy(logits, labels) # Cross entropy function, applies softmax internally\n",
    "\n",
    "    optimizer.zero_grad() # Sets gradients to 0 at start of each loop\n",
    "    loss.backward() # Computes gradients of the loss\n",
    "    optimizer.step() # Optimizer uses gradients to update parameters\n",
    "\n",
    "    ### LOGGING\n",
    "    print(f\"Epoch: {epoch+1:03d}/{num_epochs:03d}\"\n",
    "    f\" | Batch {batch_idx:03d}/{len(train_loader):03d}\"\n",
    "    f\" | Train Loss: {loss:.2f}\")\n",
    " model.eval()\n",
    " \n",
    "\n",
    "# Both the lr and the number of epochs are hyperparemters we can tune\n",
    "# In this case, we obtain convergence, loss = 0 after 3 epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93afcdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.8569, -4.1618],\n",
      "        [ 2.5382, -3.7548],\n",
      "        [ 2.0944, -3.1820],\n",
      "        [-1.4814,  1.4816],\n",
      "        [-1.7176,  1.7342]])\n",
      "tensor([[    0.9991,     0.0009],\n",
      "        [    0.9982,     0.0018],\n",
      "        [    0.9949,     0.0051],\n",
      "        [    0.0491,     0.9509],\n",
      "        [    0.0307,     0.9693]])\n",
      "tensor([0, 0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Evaluating\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    " outputs = model(X_train)\n",
    "print(outputs)\n",
    "\n",
    "# Probabilities\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "probas = torch.softmax(outputs, dim=1)\n",
    "print(probas)\n",
    "\n",
    "# For probabilities, values on the left probability of class 0, values on the right class 1, each row is an instance\n",
    "\n",
    "# To return the predictions for the instnaces\n",
    "predictions = torch.argmax(probas, dim=1) # dim = 1 returns highest value in each row\n",
    "print(predictions)\n",
    "# It is unncessary to apply softmax, as this could be done directly\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e95c23fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluating models performance\n",
    "def compute_accuracy(model, dataloader):\n",
    "    model = model.eval()\n",
    "    correct = 0.0\n",
    "    total_examples = 0\n",
    "\n",
    "    for idx, (features, labels) in enumerate(dataloader):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(features) # Obtaining outputs\n",
    "\n",
    "    predictions = torch.argmax(logits, dim=1) # Obtaining probabilities\n",
    "    compare = labels == predictions # Returns a T/F tensor is values match\n",
    "    correct += torch.sum(compare) # Counts the total number of True values\n",
    "    total_examples += len(compare)\n",
    "\n",
    "    return (correct / total_examples).item()\n",
    "\n",
    "# Obtaining accuracy for both training and testing\n",
    "print(compute_accuracy(model, train_loader))\n",
    "print(compute_accuracy(model, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d31641f",
   "metadata": {},
   "source": [
    "**A.8 Saving and loading models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a764e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"model.pth\")\n",
    "# state_dict maps each layer to its parameters\n",
    "# model.pth is an arbitrary filename\n",
    "\n",
    "# model = NeuralNetwork(2, 2) \n",
    "# important to have an instnace of the model in memory to apply the saved parameters\n",
    "#   model.load_state_dict(torch.load(\"model.pth\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9ee872",
   "metadata": {},
   "source": [
    "**A.9 Optimizing training performances with GPUs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107cdf38",
   "metadata": {},
   "source": [
    "My computer does not support this methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a611ae1",
   "metadata": {},
   "source": [
    "**SUMMARY**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761dbdc5",
   "metadata": {},
   "source": [
    " PyTorch is an open source library with three core components: a tensor library,\n",
    "automatic differentiation functions, and deep learning utilities.\n",
    "\n",
    " PyTorch’s tensor library is similar to array libraries like NumPy.\n",
    "\n",
    " In the context of PyTorch, tensors are array-like data structures representing\n",
    "scalars, vectors, matrices, and higher-dimensional arrays.\n",
    "\n",
    " PyTorch tensors can be executed on the CPU, but one major advantage of\n",
    "PyTorch’s tensor format is its GPU support to accelerate computations.\n",
    "\n",
    " The automatic differentiation (autograd) capabilities in PyTorch allow us to\n",
    "conveniently train neural networks using backpropagation without manually\n",
    "deriving gradients.\n",
    "\n",
    " The deep learning utilities in PyTorch provide building blocks for creating custom deep neural networks.\n",
    "\n",
    " PyTorch includes Dataset and DataLoader classes to set up efficient data-loading pipelines.\n",
    "\n",
    " It’s easiest to train models on a CPU or single GPU.\n",
    "\n",
    " Using DistributedDataParallel is the simplest way in PyTorch to accelerate\n",
    "the training if multiple GPUs are available."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
