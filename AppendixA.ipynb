{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4742dcf3",
   "metadata": {},
   "source": [
    "# APPENDIX A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a104bca",
   "metadata": {},
   "source": [
    "This following file covers the initiations and basics of pytorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb330241",
   "metadata": {},
   "source": [
    "**A.1 WHAT IS PYTORCH?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b91599",
   "metadata": {},
   "source": [
    "Useful due to its dealing with tensors, it automatically computes gradients for tensor operations and it has many built in loss functions and optimizers.\n",
    "Deep learning is just a type of machine learning\n",
    "\n",
    "Initializing it on the terminal\n",
    "-pip install pytorch\n",
    "-pip3 install torch torchvision torchaudio\n",
    "-pip show torch should return version 2.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1822880d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3715335f",
   "metadata": {},
   "source": [
    "**A.2 UNDERSTANDING TENSORS?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32c2acb",
   "metadata": {},
   "source": [
    "Tensors are a generalization of matrices to higher dimensions\n",
    "They are data containers for array-like structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53e342b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 0 dimensional tensor\n",
    "tensor0d = torch.tensor(1)\n",
    "\n",
    "# 1 dimensional tensor\n",
    "tensor1d = torch.tensor([1,2,3])\n",
    "\n",
    "# 2 dimensional tensor\n",
    "tensor2d = torch.tensor([[1,2],[3,4]])\n",
    "\n",
    "# 3 dimensional tensor\n",
    "tensor3d = torch.tensor([[[1,2],[3,4]],[[5,6],[7,8]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13b0224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64 torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Data Types, they are both 64-bit integers, 64 bits leads to more precision although it causes a larger memory consumption.\n",
    "print(tensor0d.dtype, tensor1d.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6aa834f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "torch.Size([2, 2])\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6],\n",
      "        [7, 8]])\n",
      "tensor([[[1, 5],\n",
      "         [3, 7]],\n",
      "\n",
      "        [[2, 6],\n",
      "         [4, 8]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_29548\\3521843475.py:14: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3701.)\n",
      "  print(tensor3d.T)\n"
     ]
    }
   ],
   "source": [
    "# Operations\n",
    "\n",
    "# Obtaining the tensor\n",
    "print(tensor0d)\n",
    "\n",
    "# Obtaining the size\n",
    "print(tensor2d.shape)\n",
    "\n",
    "# Reshaping the tensor\n",
    "print(tensor3d.reshape(4,2))\n",
    "# .view is more common on this case\n",
    "\n",
    "# Transposing the tensor\n",
    "print(tensor3d.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63538486",
   "metadata": {},
   "source": [
    "**A.3 Seeing models as computation graphs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213f1dcf",
   "metadata": {},
   "source": [
    "Autograd is a built in function of Torch which computes gradients automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a307596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0852)\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression classifier\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "y = torch.tensor([1.0]) # label\n",
    "x1 = torch.tensor([1.1]) # input\n",
    "w1 = torch.tensor([2.2]) # weight\n",
    "b = torch.tensor([0.0]) # bias\n",
    "z = x1 * w1 + b # formula\n",
    "a = torch.sigmoid(z) # activation formula, any number is squashed between 0 and 1.\n",
    "loss = F.binary_cross_entropy(a, y) # output, how wrong the prediction is\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ac4f61",
   "metadata": {},
   "source": [
    "**A.4 Automatic Differentiation Made Easy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e219c46e",
   "metadata": {},
   "source": [
    "The attribute requires_grad set to True will build a computational graph internally, this is useful if we want to compute gradients.\n",
    "Gradients are computed with partial derivatives, done using the chain rule from right to left in the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac61dae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-0.0898]),)\n",
      "(tensor([-0.0817]),)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.0817])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing gradients with autograd\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "\n",
    "\n",
    "y = torch.tensor([1.0])\n",
    "x1 = torch.tensor([1.1])\n",
    "w1 = torch.tensor([2.2], requires_grad=True) # Parameter requires grad set to True\n",
    "b = torch.tensor([0.0], requires_grad=True)\n",
    "z = x1 * w1 + b\n",
    "a = torch.sigmoid(z)\n",
    "loss = F.binary_cross_entropy(a, y)\n",
    "\n",
    "grad_L_w1 = grad(loss, w1, retain_graph=True) # Loss is a scalar value representing the model's error.\n",
    "grad_L_b = grad(loss, b, retain_graph=True) # Retain graph maintains the gradient in memory, useful if we wish to use it later\n",
    "\n",
    "print(grad_L_w1)\n",
    "print(grad_L_b)\n",
    "\n",
    "# loss.backward() does gradient computation for all parameters that have requires_grad at once. Store in .grad attributes\n",
    "loss.backward()\n",
    "b.grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f542d5a1",
   "metadata": {},
   "source": [
    "**A.5 Implementing Multilayer Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e381b07",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
