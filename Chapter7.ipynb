{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdbf64fb",
   "metadata": {},
   "source": [
    "# Fine-Tuning to follow instructions\n",
    "\n",
    "We will construct our model to follow human instructions.\n",
    "\n",
    "To do so, we require an instruction dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaec7019",
   "metadata": {},
   "source": [
    "**Preparing the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "838aee74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n",
      "Example entry:\n",
      " {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n",
      "Another example entry:\n",
      " {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"}\n"
     ]
    }
   ],
   "source": [
    "# Importing the json file\n",
    "import json\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "def download_and_load_file(file_path, url):\n",
    " if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode(\"utf-8\")\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    " else:\n",
    "    # Skips import if download already done\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()\n",
    " with open(file_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    " return data\n",
    "\n",
    "\n",
    "file_path = \"instruction-data.json\"\n",
    "url = (\n",
    " \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
    " \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    ")\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))\n",
    "\n",
    "print(\"Example entry:\\n\", data[50])\n",
    "\n",
    "print(\"Another example entry:\\n\", data[999])\n",
    "\n",
    "# All instances have an input, output and a instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de934c0b",
   "metadata": {},
   "source": [
    "**Prompt formats**\n",
    "\n",
    "There are different example formats, usually called prompt styles.\n",
    "\n",
    "Alpaca prompt style is one of the most used due to its help in defining fine-tuning \n",
    "\n",
    "This format has an input, an instruction and a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6533fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt formatting function\n",
    "def format_input(entry):\n",
    " instruction_text = (\n",
    " f\"Below is an instruction that describes a task. \"\n",
    " f\"Write a response that appropriately completes the request.\"\n",
    " f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    " )\n",
    "\n",
    " input_text = (\n",
    " f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    " )\n",
    " return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f97b48d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the correct spelling of the following word.\n",
      "\n",
      "### Input:\n",
      "Ocassion\n",
      "\n",
      "### Response:\n",
      "The correct spelling is 'Occasion.'\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is an antonym of 'complicated'?\n",
      "\n",
      "### Response:\n",
      "An antonym of 'complicated' is 'simple'.\n"
     ]
    }
   ],
   "source": [
    "# Testing our code\n",
    "model_input = format_input(data[50])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "print(model_input + desired_response)\n",
    "\n",
    "# Empty input\n",
    "model_input = format_input(data[999])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[999]['output']}\"\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1179b874",
   "metadata": {},
   "source": [
    "Partitioning the dataset into 85% training, 10%testing, 5% validating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbf8fcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 935\n",
      "Validation set length: 55\n",
      "Test set length: 110\n"
     ]
    }
   ],
   "source": [
    "# Partitioning\n",
    "# Index at which partition is done\n",
    "train_portion = int(len(data) * 0.85)\n",
    "test_portion = int(len(data) * 0.1)\n",
    "val_portion = len(data) - train_portion - test_portion\n",
    "\n",
    "# Partitioning\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]\n",
    "\n",
    "# Sets length\n",
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d466cf",
   "metadata": {},
   "source": [
    "**Training Batches**\n",
    "\n",
    "We will see how to efficiently pad the data samples to equal lengths to assemble multiple instruction examples in a batch.\n",
    "\n",
    "The DataLoader function in Pytorch uses a collate function. A collate function is responsible for taking a list of individual data samples and merging them into a single batch that can be processed efficiently by the\n",
    "model during training\n",
    "\n",
    "Howevever, as we are dealing with instructions, we have to make our own collate function.\n",
    "\n",
    "Firstly, we create an Instruction Dataset class which applies format input and pretokenizes all inputs. The first function formats the input to be an instruction-response template. Then, we will tokenize the input, ensuring to padd the length when strictly necessary. We will then create target token IDs for training (inputs shifted by 1). Finally, we can apply mask padding to some of the padding tokens to exclude from loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a6a06e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction dataset class\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    " def __init__(self, data, tokenizer):\n",
    "    self.data = data # List of dictionaries\n",
    "    self.encoded_texts = [] # Stores tokenized inputs\n",
    "    for entry in data:\n",
    "        instruction_plus_input = format_input(entry) # Formatted instruction+input string\n",
    "        response_text = f\"\\n\\n### Response:\\n{entry['output']}\" # Add a response string\n",
    "        full_text = instruction_plus_input + response_text # Add all texts together\n",
    "        self.encoded_texts.append(  # Add the encoded text\n",
    "        tokenizer.encode(full_text)\n",
    "        )\n",
    " \n",
    " # Retrieve an item\n",
    " def __getitem__(self, index):\n",
    "    return self.encoded_texts[index]\n",
    " \n",
    " # Legnth of the data\n",
    " def __len__(self):\n",
    "    return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d9994a",
   "metadata": {},
   "source": [
    "To pad all intputs to the same length, we will use the same token as Chapter6 endoftext, which value is 50256.\n",
    "\n",
    "The collate function which we will now implement, ensures that all elements have fied length inside of their batch, not with respect to the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e57b3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function\n",
    "# We add one token at the end (usually EOS or PAD), \n",
    "# and then remove the last token when building the input — this is done to shift the inputs and targets for training a language model.\n",
    "# If input [20,30,50] we padd to have [20,30,50,50256], target will be [30,50,50256], something that would not happen if we did not\n",
    "# pad at the beginning\n",
    "\n",
    "def custom_collate_draft_1(\n",
    " batch,\n",
    " pad_token_id=50256,\n",
    " device=\"cpu\"\n",
    "):\n",
    " \"\"\"Custom collate function\n",
    " batch: a list of sequences from the dataset\n",
    " pad_token_id: token to pad\n",
    " device: CPU or GPU\"\"\"\n",
    "\n",
    " # Maximum batch length\n",
    " batch_max_length = max(len(item)+1 for item in batch)\n",
    " # Initiates inputs list\n",
    " inputs_lst = []\n",
    " for item in batch:\n",
    "    new_item = item.copy()\n",
    "    # Adds a token at the end\n",
    "    new_item += [pad_token_id]\n",
    "    # Pads all the inputs to max length\n",
    "    padded = (\n",
    "    new_item + [pad_token_id] *\n",
    "    (batch_max_length - len(new_item))\n",
    "    )\n",
    "    # Removes last padded input\n",
    "    inputs = torch.tensor(padded[:-1])\n",
    "    # Stores the input\n",
    "    inputs_lst.append(inputs)\n",
    "\n",
    " # Tensor with all inputs\n",
    " inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    " return inputs_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a57e1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "# Testing the code\n",
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "batch = (\n",
    " inputs_1,\n",
    " inputs_2,\n",
    " inputs_3\n",
    ")\n",
    "print(custom_collate_draft_1(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89c2e72",
   "metadata": {},
   "source": [
    "We now need to consider target tokens, this is why we have to modify our current function to incorporate target tokenId.\n",
    "\n",
    "What this does, is to shift the input by one element and adds an end of sequence to the end of the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03d3a6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "Targets: tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256, 50256, 50256, 50256],\n",
      "        [    8,     9, 50256, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "# Second collate function\n",
    "def custom_collate_draft_2(\n",
    " batch,\n",
    " pad_token_id=50256,\n",
    " device=\"cpu\"\n",
    "):\n",
    " # Max batch length\n",
    " batch_max_length = max(len(item)+1 for item in batch)\n",
    " # Initiates empty inputs and target lists\n",
    " inputs_lst, targets_lst = [], []\n",
    " for item in batch:\n",
    "    new_item = item.copy()\n",
    "    new_item += [pad_token_id] # Pad at the end\n",
    "    padded = ( # Pads input elements to max length\n",
    "    new_item + [pad_token_id] *\n",
    "    (batch_max_length - len(new_item)))\n",
    "    inputs = torch.tensor(padded[:-1]) # Removes last element\n",
    "    targets = torch.tensor(padded[1:]) # Removes first (shifts to the right)\n",
    "    # Stores inputs and targets\n",
    "    inputs_lst.append(inputs)\n",
    "    targets_lst.append(targets)\n",
    " # Converts values to tensors\n",
    " inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    " targets_tensor = torch.stack(targets_lst).to(device)\n",
    " return inputs_tensor, targets_tensor\n",
    "\n",
    "# Running the code\n",
    "inputs, targets = custom_collate_draft_2(batch)\n",
    "print(\"Inputs:\",inputs)\n",
    "print(\"Targets:\",targets)\n",
    "# We see our model is indeed working correctly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd1cb0c",
   "metadata": {},
   "source": [
    "We will modify our collate function again, by replacing the last end of sequence tokens by -100.\n",
    "\n",
    "In this way, the model will not be influence by irrelevant data and only meaningful data actually have an influence in model learning.\n",
    "\n",
    "However, we will still have an end-of-text token which will be used as an indicator that the response is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c54a81b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256,  -100,  -100,  -100],\n",
      "        [    8,     9, 50256,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "# Final collate function\n",
    "def custom_collate_fn(\n",
    " batch,\n",
    " pad_token_id=50256,\n",
    " ignore_index=-100,\n",
    " allowed_max_length=None,\n",
    " device=\"cpu\"):\n",
    "   \n",
    " \"\"\"Custom collate function\n",
    " batch: a list of sequences from the dataset\n",
    " pad_token_id: token to pad\n",
    " device: CPU or GPU\n",
    " allowed-max_length: maximum allowed amount to a token\"\"\"\n",
    " \n",
    " # Maximum allowed length to a batch\n",
    " batch_max_length = max(len(item)+1 for item in batch)\n",
    " # Empty lists for storing inputs and targets\n",
    " inputs_lst, targets_lst = [], []\n",
    "\n",
    " # Iterating through batch loop\n",
    " for item in batch:\n",
    "    new_item = item.copy()\n",
    "    new_item += [pad_token_id] # End of sequence token\n",
    "\n",
    "    padded = ( # Pad elements to max length\n",
    "    new_item + [pad_token_id] *\n",
    "    (batch_max_length - len(new_item))\n",
    "    )\n",
    "    # Removes last element\n",
    "    inputs = torch.tensor(padded[:-1])\n",
    "    targets = torch.tensor(padded[1:]) # Removes first element (shift by 1)\n",
    "    \n",
    "    # Masking    \n",
    "    mask = targets == pad_token_id # Maps mask to the desired token\n",
    "    indices = torch.nonzero(mask).squeeze() \n",
    "    if indices.numel() > 1:\n",
    "        targets[indices[1:]] = ignore_index # Ignore indices equal to the desired token except for the first one\n",
    "\n",
    "    # Optionally truncates to maximum allowed length\n",
    "    if allowed_max_length is not None:\n",
    "        inputs = inputs[:allowed_max_length]\n",
    "        targets = targets[:allowed_max_length]\n",
    "\n",
    "    # Store inputs and targets\n",
    "    inputs_lst.append(inputs)\n",
    "    targets_lst.append(targets)\n",
    "\n",
    " # Converts inputs and targets to tensors\n",
    " inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    " targets_tensor = torch.stack(targets_lst).to(device)\n",
    " return inputs_tensor, targets_tensor\n",
    "\n",
    "# Testing our code\n",
    "inputs, targets = custom_collate_fn(batch)\n",
    "print(inputs)\n",
    "print(targets)\n",
    "# We see our code is working correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4541c4be",
   "metadata": {},
   "source": [
    "Our function ignores this index as by definition the cross entropy loss function ignores these indices.\n",
    "\n",
    "One optional approach would be to mask the tokens of the instruction as well, however, this will not be covered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da0b8cc",
   "metadata": {},
   "source": [
    "**DataLoaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1b6c70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Obtaining the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# Collate function with the desired device\n",
    "customized_collate_fn = partial(\n",
    " custom_collate_fn,\n",
    " device=device,\n",
    " allowed_max_length=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33b091c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataLoaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Obtaining tokenizer\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Initial parameters\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "# Training dataset and DataLoader\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    " train_dataset,\n",
    " batch_size=batch_size,\n",
    " collate_fn=customized_collate_fn,\n",
    " shuffle=True,\n",
    " drop_last=True,\n",
    " num_workers=num_workers\n",
    ")\n",
    "\n",
    "# Validation dataset and DataLoader\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    " val_dataset,\n",
    " batch_size=batch_size,\n",
    " collate_fn=customized_collate_fn,\n",
    " shuffle=False,\n",
    " drop_last=False,\n",
    " num_workers=num_workers\n",
    ")\n",
    "\n",
    "# Testing dataset and DataLoader\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    " test_dataset,\n",
    " batch_size=batch_size,\n",
    " collate_fn=customized_collate_fn,\n",
    " shuffle=False,\n",
    " drop_last=False,\n",
    " num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88921ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 89]) torch.Size([8, 89])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 88]) torch.Size([8, 88])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 58]) torch.Size([8, 58])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 87]) torch.Size([8, 87])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 57]) torch.Size([8, 57])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 82]) torch.Size([8, 82])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n"
     ]
    }
   ],
   "source": [
    "# Exploring dimensions\n",
    "print(\"Train loader:\")\n",
    "for inputs, targets in train_loader:\n",
    " print(inputs.shape, targets.shape)\n",
    "\n",
    " # The 8 represents the batch size, the number to the right, the length of each input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c74266",
   "metadata": {},
   "source": [
    "**Pretrained LLM**\n",
    "\n",
    "We will use the medium-size model with 330 million parameters as smaller models lack capacity to achieve correct results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab44e9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\355M\\checkpoint\n",
      "File already exists and is up-to-date: gpt2\\355M\\encoder.json\n",
      "File already exists and is up-to-date: gpt2\\355M\\hparams.json\n",
      "File already exists and is up-to-date: gpt2\\355M\\model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2\\355M\\model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2\\355M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2\\355M\\vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "# Loading pretrained model\n",
    "from gpt_download import download_and_load_gpt2\n",
    "from chapter04 import GPTModel\n",
    "from chapter05 import load_weights_into_gpt\n",
    "\n",
    "\n",
    "BASE_CONFIG = {\n",
    " \"vocab_size\": 50257, # Vocabulary size\n",
    " \"context_length\": 1024, # Context length\n",
    " \"drop_rate\": 0.0, # Dropout rate\n",
    " \"qkv_bias\": True # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    " \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    " \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    " \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    " \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "# Medium model\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(\n",
    " model_size=model_size,\n",
    " models_dir=\"gpt2\"\n",
    ")\n",
    "\n",
    "# Initiate model\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "# Loading weights\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f820afd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
      "Response text: ### Response:\n",
      "\n",
      "The chef cooks the meal every day.\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "Convert the active sentence to passive: 'The chef cooks the\n"
     ]
    }
   ],
   "source": [
    "# Assesing the models performance\n",
    "torch.manual_seed(123)\n",
    "input_text = format_input(val_data[0])\n",
    "print(input_text)\n",
    "\n",
    "# Generating models response with the generate function we had\n",
    "from chapter05 import generate, text_to_token_ids, token_ids_to_text\n",
    "token_ids = generate(\n",
    " model=model,\n",
    " idx=text_to_token_ids(input_text, tokenizer),\n",
    " max_new_tokens=35,\n",
    " context_size=BASE_CONFIG[\"context_length\"],\n",
    " eos_id=50256,\n",
    ")\n",
    "generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "\n",
    "# Now, we have to separate input and output, as for our previous model this was done, but we dont need it now\n",
    "response_text = generated_text[len(input_text):].strip()\n",
    "print(\"Response text:\",response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25c5d93",
   "metadata": {},
   "source": [
    "Our model has yet to make some corrections, this is why we will proceed to fine-tune the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b911168a",
   "metadata": {},
   "source": [
    "**Fine-tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4167ecbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.8258963584899903\n",
      "Validation loss: 3.7619213581085207\n"
     ]
    }
   ],
   "source": [
    "# Importing loss functions\n",
    "from chapter05 import (\n",
    " calc_loss_loader,\n",
    " train_model_simple\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "torch.manual_seed(123)\n",
    "with torch.no_grad():\n",
    " train_loss = calc_loss_loader(\n",
    " train_loader, model, device, num_batches=5\n",
    " )\n",
    " val_loss = calc_loss_loader(\n",
    " val_loader, model, device, num_batches=5\n",
    ")\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa66f312",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b056d12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 2.637, Val loss 2.626\n",
      "Ep 1 (Step 000005): Train loss 1.174, Val loss 1.102\n",
      "Ep 1 (Step 000010): Train loss 0.872, Val loss 0.945\n",
      "Ep 1 (Step 000015): Train loss 0.856, Val loss 0.906\n",
      "Ep 1 (Step 000020): Train loss 0.776, Val loss 0.881\n",
      "Ep 1 (Step 000025): Train loss 0.753, Val loss 0.859\n",
      "Ep 1 (Step 000030): Train loss 0.798, Val loss 0.836\n",
      "Ep 1 (Step 000035): Train loss 0.714, Val loss 0.808\n",
      "Ep 1 (Step 000040): Train loss 0.672, Val loss 0.806\n",
      "Ep 1 (Step 000045): Train loss 0.633, Val loss 0.790\n",
      "Ep 1 (Step 000050): Train loss 0.662, Val loss 0.783\n",
      "Ep 1 (Step 000055): Train loss 0.760, Val loss 0.764\n",
      "Ep 1 (Step 000060): Train loss 0.719, Val loss 0.743\n",
      "Ep 1 (Step 000065): Train loss 0.652, Val loss 0.735\n",
      "Ep 1 (Step 000070): Train loss 0.532, Val loss 0.729\n",
      "Ep 1 (Step 000075): Train loss 0.569, Val loss 0.729\n",
      "Ep 1 (Step 000080): Train loss 0.605, Val loss 0.725\n",
      "Ep 1 (Step 000085): Train loss 0.509, Val loss 0.709\n",
      "Ep 1 (Step 000090): Train loss 0.562, Val loss 0.691\n",
      "Ep 1 (Step 000095): Train loss 0.500, Val loss 0.681\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    " model.parameters(), lr=0.00005, weight_decay=0.1\n",
    ")\n",
    "# Epochs\n",
    "num_epochs = 2\n",
    "# Training\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    " model, train_loader, val_loader, optimizer, device,\n",
    " num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    " start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bf6753",
   "metadata": {},
   "source": [
    "The training output shows that the model is learning effectively, as we can tell based\n",
    "on the consistently decreasing training and validation loss values over the two epochs.\n",
    "This result suggests that the model is gradually improving its ability to understand and\n",
    "follow the provided instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419929d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting losses for more understanding\n",
    "from chapter05 import plot_losses_epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4844bbf",
   "metadata": {},
   "source": [
    "we can see that the model’s performance on\n",
    "both the training and validation sets improves substantially over the course of training. The rapid decrease in losses during the initial phase indicates that the model\n",
    "quickly learns meaningful patterns and representations from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baeefbb9",
   "metadata": {},
   "source": [
    "**Extracting and saving responses**\n",
    "\n",
    "We now extract the responses from our models and compare them to the actual answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d01fdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "for entry in test_data[:3]: # 3 first examples \n",
    "    input_text = format_input(entry) # Formats inouts\n",
    "    token_ids = generate( # Tokenize input\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "    max_new_tokens=256,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer) # Token IDs to text\n",
    "\n",
    "    response_text = ( # Extract just the response, avoiding the input\n",
    "    generated_text[len(input_text):]\n",
    "    .replace(\"### Response:\", \"\")\n",
    "    .strip()\n",
    "    )\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
    "    print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6ed530",
   "metadata": {},
   "source": [
    "The way to measure and evaluate our models performance is actually complicated. This is due to that the answer can be correct although not the same as the models.\n",
    "\n",
    "There are different approaches for this.\n",
    "\n",
    "In practice, it can be useful to consider all three types of evaluation methods: multiplechoice question answering, human evaluation, and automated metrics(Metrics that automatically assess the dialogue quality) that measure\n",
    "conversational performance.\n",
    "\n",
    "However, since we are primarily interested in assessing conversational performance rather than just the ability to answer multiple-choice questions, human evaluation and automated metrics may be more relevant\n",
    "\n",
    "Conversational performance\n",
    "Conversational performance of LLMs refers to their ability to engage in human-like\n",
    "communication by understanding context, nuance, and intent. It encompasses skills\n",
    "such as providing relevant and coherent responses, maintaining consistency, and\n",
    "adapting to different topics and styles of interaction.\n",
    "\n",
    "We will use another LLM to evaluate this responses\n",
    "\n",
    "\n",
    "We will use our own custom test set and save the updated data\n",
    "\n",
    "To prepare the responses for this evaluation process, we append the generated\n",
    "model responses to the test_set dictionary and save the updated data as an\n",
    "\"instruction-data-with-response.json\" file for record keeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94137ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating test set responses\n",
    "from tqdm import tqdm\n",
    "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    " input_text = format_input(entry)\n",
    " token_ids = generate(\n",
    " model=model,\n",
    " idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    " max_new_tokens=256,\n",
    " context_size=BASE_CONFIG[\"context_length\"],\n",
    " eos_id=50256\n",
    " )\n",
    " generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "\n",
    " response_text = (\n",
    " generated_text[len(input_text):]\n",
    " .replace(\"### Response:\", \"\")\n",
    " .strip()\n",
    " )\n",
    " test_data[i][\"model_response\"] = response_text\n",
    " \n",
    "with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
    " json.dump(test_data, file, indent=4) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7b1cfc",
   "metadata": {},
   "source": [
    "To do this, Ollama application is used, however, i avoided this.\n",
    "Takes 5 GB\n",
    "\n",
    "It is basically another LLM model which provides corrections and answers based on the results obtained from the previous steps.\n",
    "\n",
    "To further improve our model’s performance, we can explore various strategies,\n",
    "such as\n",
    "\n",
    " Adjusting the hyperparameters during fine-tuning, such as the learning rate,\n",
    "batch size, or number of epochs\n",
    "\n",
    " Increasing the size of the training dataset or diversifying the examples to cover\n",
    "a broader range of topics and styles\n",
    "\n",
    " Experimenting with different prompts or instruction formats to guide the\n",
    "model’s responses more effectively\n",
    "\n",
    " Using a larger pretrained model, which may have greater capacity to capture\n",
    "complex patterns and generate more accurate responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edcea3c",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "\n",
    " The instruction-fine-tuning process adapts a pretrained LLM to follow human\n",
    "instructions and generate desired responses.\n",
    "\n",
    " Preparing the dataset involves downloading an instruction-response dataset,\n",
    "formatting the entries, and splitting it into train, validation, and test sets.\n",
    "\n",
    " Training batches are constructed using a custom collate function that pads\n",
    "sequences, creates target token IDs, and masks padding tokens.\n",
    "\n",
    " We load a pretrained GPT-2 medium model with 355 million parameters to\n",
    "serve as the starting point for instruction fine-tuning.\n",
    "\n",
    " The pretrained model is fine-tuned on the instruction dataset using a training\n",
    "loop similar to pretraining.\n",
    "\n",
    " Evaluation involves extracting model responses on a test set and scoring them\n",
    "(for example, using another LLM).\n",
    "\n",
    " The Ollama application with an 8-billion-parameter Llama model can be used\n",
    "to automatically score the fine-tuned model’s responses on the test set, providing an average score to quantify performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
