{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbe49229",
   "metadata": {},
   "source": [
    "# Working with text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are currently at step 1, data preparation and sampling.\n",
    "\n",
    "To prepare input texts, we need to separate it into individual words and tokens to be able to encode them.\n",
    "\n",
    "Embedding refers to the process of converting data in this case text into a vector format.\n",
    "\n",
    "The purpose is to have a data format which neural networks can process\n",
    "\n",
    "There are different embeddings, however we will focus on words embeddings as we want to generate one at a time.\n",
    "\n",
    "Word embeddings can have varying dimensions, from one to thousands. A higher\n",
    "dimensionality might capture more nuanced relationships but at the cost of computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78c39a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n",
      "Separating words:  ['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n",
      "Separating dots and commas:  ['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n",
      "Removing blank spaces:  ['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n",
      "With punctuation:  ['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "# To practice this, we will use the-verdict.txt file\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    " raw_text = f.read()\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])\n",
    "\n",
    "# We wish to turn all this characters into tokens which we can embedd\n",
    "\n",
    "# To obtain the different set of characters we use the re library\n",
    "import re\n",
    "\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "print(\"Separating words: \",result)\n",
    "\n",
    "# We wish to separate dots and commas to separate instances\n",
    "result = re.split(r'([,.]|\\s)', text)\n",
    "print(\"Separating dots and commas: \",result)\n",
    "\n",
    "# If we wish to remove blank space characters\n",
    "result = [item for item in result if item.strip()]\n",
    "print(\"Removing blank spaces: \",result)\n",
    "\n",
    "# Removing white spaces can depend on what the focus is as it can be memory efficient or needed to avoid erros.\n",
    "\n",
    "# Taking into account all punctuaction terms\n",
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(\"With punctuation: \", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4228f7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of tokens (no whitespaces):  4690\n",
      "First 30 tokens:  ['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "# Applying this to our whole text\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(\"Total amount of tokens (no whitespaces): \",len(preprocessed))\n",
    "\n",
    "print(\"First 30 tokens: \",preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d01e385",
   "metadata": {},
   "source": [
    "We need to provide token ID, in other words, assign each token to a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35deb785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n",
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed)) # Set obtains unique tokens, sorted orders them in alphabetical order\n",
    "print(len(all_words)) \n",
    "\n",
    "# Printing the first 51 elements\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    " print(item)\n",
    " if i >= 50:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153d82c9",
   "metadata": {},
   "source": [
    "We will create a class that both encodes words into tokens and thus TokenID and a decoder to reverse this operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a2efaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    " def __init__(self, vocab):\n",
    "    self.str_to_int = vocab # Maps strings to tokens\n",
    "    self.int_to_str = {i:s for s,i in vocab.items()} # Reverse mapping\n",
    "\n",
    " def encode(self, text):\n",
    "    preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text) # Tokenizes items\n",
    "    preprocessed = [\n",
    "    item.strip() for item in preprocessed if item.strip()\n",
    "    ] # Ensures empty spaces are cleaned\n",
    "    ids = [self.str_to_int[s] for s in preprocessed] # Converts each token into its integer ID\n",
    "    return ids\n",
    "\n",
    " def decode(self, ids):\n",
    "    text = \" \".join([self.int_to_str[i] for i in ids]) # Integer to string\n",
    "\n",
    "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) # Joins strings with a space\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0e9f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID:  [1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
      "Texts decoded:  \" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# As this works, trying with a different training set\u001b[39;00m\n\u001b[32m     10\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mHello, do you like tea?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mSimpleTokenizerV1.encode\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m      7\u001b[39m preprocessed = re.split(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m([,.?_!\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33m]|--|\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms)\u001b[39m\u001b[33m'\u001b[39m, text) \u001b[38;5;66;03m# Tokenizes items\u001b[39;00m\n\u001b[32m      8\u001b[39m preprocessed = [\n\u001b[32m      9\u001b[39m item.strip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item.strip()\n\u001b[32m     10\u001b[39m ] \u001b[38;5;66;03m# Ensures empty spaces are cleaned\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m ids = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpreprocessed\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;66;03m# Converts each token into its integer ID\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      7\u001b[39m preprocessed = re.split(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m([,.?_!\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33m]|--|\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms)\u001b[39m\u001b[33m'\u001b[39m, text) \u001b[38;5;66;03m# Tokenizes items\u001b[39;00m\n\u001b[32m      8\u001b[39m preprocessed = [\n\u001b[32m      9\u001b[39m item.strip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item.strip()\n\u001b[32m     10\u001b[39m ] \u001b[38;5;66;03m# Ensures empty spaces are cleaned\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m ids = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed] \u001b[38;5;66;03m# Converts each token into its integer ID\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[31mKeyError\u001b[39m: 'Hello'"
     ]
    }
   ],
   "source": [
    "# Trying this class with a small subtext\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    " Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(\"Token ID: \",ids)\n",
    "print(\"Texts decoded: \", tokenizer.decode(ids))\n",
    "\n",
    "# As this works, trying with a different training set\n",
    "text = \"Hello, do you like tea?\"\n",
    "# print(tokenizer.encode(text))\n",
    "\n",
    "# Error, due to Hello not appearing on the original text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e3f679",
   "metadata": {},
   "source": [
    "We need to make changes to adapt to unknown words, we will modify vocabulary and tokenizers\n",
    "\n",
    "Special tokenizers will handle this\n",
    "\n",
    "We can create a tokenizer which handles unknow words, and another for unrelated texts. The latter helps as if we insert independent texts, they are presented in a single manner, however they are actually unrelated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f15d182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"]) # Adding the two newest tokens\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "print(len(vocab.items()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
